# ============================================
# GestureGPT Demo Configuration
# ============================================
# Copy this file to .env and configure for your setup

# ============================================
# LLM Provider Configuration
# ============================================
# Choose one: placeholder, openai, anthropic, custom
# - placeholder: No API key needed, uses canned responses (default)
# - openai: OpenAI GPT models
# - anthropic: Anthropic Claude models
# - custom: Local LLM (Ollama, LM Studio, etc.)

LLM_PROVIDER=placeholder

# ============================================
# OpenAI Configuration
# ============================================
# Get API key from: https://platform.openai.com/api-keys
# Uncomment and configure if using LLM_PROVIDER=openai

# OPENAI_API_KEY=sk-proj-your-key-here
# OPENAI_MODEL=gpt-3.5-turbo
# OPENAI_BASE_URL=https://api.openai.com/v1

# ============================================
# Anthropic Claude Configuration
# ============================================
# Get API key from: https://console.anthropic.com/
# Uncomment and configure if using LLM_PROVIDER=anthropic

# ANTHROPIC_API_KEY=sk-ant-your-key-here
# ANTHROPIC_MODEL=claude-3-haiku-20240307

# ============================================
# Local LLM Configuration (Ollama/LM Studio)
# ============================================
# Uncomment and configure if using LLM_PROVIDER=custom

# For LM Studio (see docs/LM_STUDIO_SETUP.md):
# CUSTOM_LLM_ENDPOINT=http://host.docker.internal:1234/v1/chat/completions
# CUSTOM_LLM_MODEL=local-model

# For Ollama:
# CUSTOM_LLM_ENDPOINT=http://host.docker.internal:11434/v1/chat/completions
# CUSTOM_LLM_MODEL=llama2

# Note: Use host.docker.internal for Docker on Windows/Mac
#       Use localhost or host IP for Linux with network_mode: host
